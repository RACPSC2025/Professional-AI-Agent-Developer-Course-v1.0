# M√≥dulo 10: Full Stack Agentic Apps

![Module 10 Header](../images/module10_banner.png)

![Level](https://img.shields.io/badge/Nivel-Enterprise-2ECC71?style=for-the-badge&logo=fastapi&logoColor=white)
![Time](https://img.shields.io/badge/Tiempo-8_Horas-A7C7E7?style=for-the-badge&labelColor=2D2D44)
![Stack](https://img.shields.io/badge/Stack-FastAPI_|_Streamlit_|_Docker_|_Redis-2ECC71?style=for-the-badge)

> *"Un script de Python en tu laptop es un experimento. Una API as√≠ncrona en Kubernetes es un producto."*

---

## üéØ Objetivos del M√≥dulo

Hemos pasado 9 m√≥dulos construyendo cerebros. Ahora vamos a construir el cuerpo.
En este m√≥dulo, transformaremos tu agente en una **Aplicaci√≥n Full Stack** capaz de escalar a miles de usuarios.

**Lo que vas a dominar:**
1.  üèóÔ∏è **Arquitectura Enterprise:** Dise√±o de sistemas desacoplados (Frontend, Backend, Worker).
2.  ‚ö° **Concurrencia Real:** Por qu√© `async def` no es opcional en IA.
3.  ÔøΩÔ∏è **Producci√≥n:** Inyecci√≥n de dependencias, manejo de errores y configuraci√≥n robusta.

---

## üìö 1. La Arquitectura "Agentic Stack"

Para salir de `localhost`, necesitamos dividir responsabilidades.

### El Problema del "Script √önico"
Si pones tu UI (Streamlit) y tu l√≥gica (LangChain) en el mismo archivo:
-   ‚ùå **No escala:** Un usuario pesado bloquea la UI para todos.
-   ‚ùå **No es seguro:** Tus API Keys viven en el navegador del cliente.
-   ‚ùå **No es mantenible:** Mezclas HTML con l√≥gica de grafos.

### La Soluci√≥n: Arquitectura de 3 Capas

```mermaid
graph LR
    User((üë§ Usuario)) -->|HTTPS| Frontend[üíª Frontend UI]
    Frontend -->|REST / WS| Backend[üöÄ Backend API]
    
    subgraph "Zona Segura (Servidor)"
        Backend -->|Job| Queue[(‚ö° Task Queue)]
        Backend -->|Stream| LLM[ÔøΩ GPT-4 / Claude]
        Queue --> Worker[ÔøΩ Background Worker]
    end
    
    style Backend fill:#2ECC71,color:#fff
    style Queue fill:#F39C12,color:#fff
    style Frontend fill:#3498DB,color:#fff
```

---

## ‚ö° 2. Masterclass de Concurrencia: Async vs Sync

En el desarrollo de Agentes, la **latencia** es el enemigo.
GPT-4 tarda ~10 segundos en responder. Si tu servidor es s√≠ncrono (como Flask est√°ndar), durante esos 10 segundos **tu servidor est√° muerto** para otros usuarios.

### La Analog√≠a de la Pizzer√≠a üçï

#### üê¢ Enfoque S√≠ncrono (Bloqueante)
1.  Cliente A pide pizza.
2.  Cajero va a la cocina, **se queda mirando el horno 10 minutos**.
3.  Entrega pizza A.
4.  Reci√©n atiende al Cliente B.
*Resultado:* El Cliente B espera 10 minutos solo para pedir.

#### üêá Enfoque As√≠ncrono (Non-blocking)
1.  Cliente A pide pizza.
2.  Cajero pasa la nota a la cocina y le da un "Beeper" al Cliente A.
3.  **Inmediatamente** atiende al Cliente B.
4.  Cuando la pizza A est√° lista, el Beeper suena.
*Resultado:* El cajero (CPU) nunca est√° ocioso.

### Implementaci√≥n en Python

```python
# ‚ùå MAL: Bloquea el servidor
import time
def chat_sync(message):
    response = call_gpt4(message) # Tarda 5s
    return response

# ‚úÖ BIEN: Libera el servidor
import asyncio
async def chat_async(message):
    response = await call_gpt4_async(message) # Libera el control mientras espera
    return response
```

---

## üèóÔ∏è 3. Construyendo el Backend (Paso a Paso)

Vamos a usar **FastAPI**, el est√°ndar de oro para APIs de IA.

### Paso 1: Definir el Modelo de Datos
Usamos **Pydantic** para validar que lo que entra es correcto. Si el usuario no env√≠a `query`, la API rechaza la petici√≥n autom√°ticamente.

```python
from pydantic import BaseModel

class AgentRequest(BaseModel):
    query: str
    user_id: str = "guest_user"
    temperature: float = 0.7
```

### Paso 2: Streaming de Respuesta (SSE)
Los usuarios odian esperar. Usamos **Server-Sent Events (SSE)** para enviar la respuesta palabra por palabra, igual que ChatGPT.

```python
from fastapi.responses import StreamingResponse

@app.post("/chat")
async def chat_endpoint(req: AgentRequest):
    return StreamingResponse(
        agent_generator(req.query), # Generador as√≠ncrono
        media_type="text/event-stream"
    )
```

---

## üõ†Ô∏è Proyectos Pr√°cticos (Nivel Enterprise)

### üöÄ Proyecto 1: Backend API Robusto
**Archivo:** [`01_agent_api.py`](01_agent_api.py)
Este no es un "Hello World". Es una base s√≥lida para producci√≥n:
-   ‚úÖ **Inyecci√≥n de Dependencias:** Para gestionar la configuraci√≥n y servicios.
-   ‚úÖ **Middleware:** CORS y Logging de tiempo de respuesta.
-   ‚úÖ **Manejo de Errores Global:** Captura excepciones y devuelve JSONs limpios.
-   ‚úÖ **Streaming Real:** Conexi√≥n as√≠ncrona con LangChain.

### üé® Proyecto 2: Frontend Profesional
**Archivo:** [`02_agent_ui.py`](02_agent_ui.py)
Una interfaz en Streamlit que se siente como una App nativa:
-   ‚úÖ **Gesti√≥n de Sesi√≥n:** Recuerda el historial.
-   ‚úÖ **Configuraci√≥n en Sidebar:** Ajusta temperatura y modelo.
-   ‚úÖ **Feedback Visual:** Indicadores de carga y streaming fluido.

---

## üìä El Stack Ganador 2025

Si vas a construir esto para una empresa, este es el stack recomendado:

| Capa | Tecnolog√≠a | Por qu√© |
| :--- | :--- | :--- |
| **Lenguaje** | **Python 3.11+** | Tipado fuerte, r√°pido, ecosistema IA. |
| **API Framework** | **FastAPI** | Async nativo, validaci√≥n autom√°tica, docs (Swagger). |
| **Frontend** | **Next.js (React)** | Para apps complejas. Usa **Streamlit** solo para demos. |
| **Orquestaci√≥n** | **LangGraph** | Control de estado superior a cadenas simples. |
| **Cola de Tareas** | **Celery + Redis** | Para tareas que duran >30s (investigaci√≥n profunda). |
| **Contenedores** | **Docker** | "Funciona en mi m√°quina" -> Funciona en la nube. |

---

<div align="center">

**[‚¨ÖÔ∏è M√≥dulo Anterior](../module9/README.md)** | **[üè† Inicio](../README.md)** | **[Siguiente M√≥dulo ‚û°Ô∏è](../module11/README.md)**

</div>
