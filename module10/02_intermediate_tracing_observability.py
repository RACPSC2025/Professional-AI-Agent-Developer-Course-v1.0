"""
MÃ³dulo 10 - Ejemplo Intermedio: Sistema de Tracing y Observabilidad
Framework: LangSmith + LangChain
Caso de uso: Dashboard de monitoreo en tiempo real

Este sistema demuestra cÃ³mo implementar observabilidad completa con mÃ©tricas
de latencia, costo, y calidad de respuestas.

InstalaciÃ³n:
pip install langchain langchain-openai langsmith
"""

import os
import time
from typing import Dict, List
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from dotenv import load_dotenv

load_dotenv()

# Configurar LangSmith (opcional pero recomendado para producciÃ³n)
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "production-monitoring"
# AsegÃºrate de tener LANGCHAIN_API_KEY configurada


class ProductionMonitor:
    """Monitor de mÃ©tricas de producciÃ³n"""
    
    def __init__(self):
        self.metrics = {
            "total_requests": 0,
            "total_tokens": 0,
            "total_cost_usd": 0.0,
            "total_latency_ms": 0.0,
            "errors": 0,
            "requests_by_model": {},
        }
    
    def record_request(self, model: str, tokens: int, latency_ms: float, cost_usd: float, error: bool = False):
        """Registrar una peticiÃ³n"""
        self.metrics["total_requests"] += 1
        self.metrics["total_tokens"] += tokens
        self.metrics["total_cost_usd"] += cost_usd
        self.metrics["total_latency_ms"] += latency_ms
        
        if error:
            self.metrics["errors"] += 1
        
        if model not in self.metrics["requests_by_model"]:
            self.metrics["requests_by_model"][model] = 0
        self.metrics["requests_by_model"][model] += 1
    
    def get_summary(self) -> Dict:
        """Obtener resumen de mÃ©tricas"""
        avg_latency = (self.metrics["total_latency_ms"] / self.metrics["total_requests"] 
                      if self.metrics["total_requests"] > 0 else 0)
        
        avg_tokens = (self.metrics["total_tokens"] / self.metrics["total_requests"]
                     if self.metrics["total_requests"] > 0 else 0)
        
        error_rate = (self.metrics["errors"] / self.metrics["total_requests"] * 100
                     if self.metrics["total_requests"] > 0 else 0)
        
        return {
            **self.metrics,
            "avg_latency_ms": avg_latency,
            "avg_tokens_per_request": avg_tokens,
            "error_rate_percent": error_rate
        }
    
    def print_dashboard(self):
        """Imprimir dashboard de mÃ©tricas"""
        summary = self.get_summary()
        
        print("\n" + "=" * 70)
        print("ğŸ“Š PRODUCTION METRICS DASHBOARD")
        print("=" * 70)
        print(f"\nğŸ“ˆ Request Statistics:")
        print(f"   Total Requests: {summary['total_requests']}")
        print(f"   Errors: {summary['errors']} ({summary['error_rate_percent']:.2f}% error rate)")
        
        print(f"\nâ±ï¸ Performance:")
        print(f"   Avg Latency: {summary['avg_latency_ms']:.0f} ms")
        print(f"   Total Latency: {summary['total_latency_ms']/1000:.2f} seconds")
        
        print(f"\nğŸ’° Cost Metrics:")
        print(f"   Total Cost: ${summary['total_cost_usd']:.4f}")
        print(f"   Total Tokens: {summary['total_tokens']:,}")
        print(f"   Avg Tokens/Request: {summary['avg_tokens_per_request']:.0f}")
        print(f"   Cost per 1K tokens: ${summary['total_cost_usd']/(summary['total_tokens']/1000):.4f}" 
              if summary['total_tokens'] > 0 else "")
        
        print(f"\nğŸ¤– Model Distribution:")
        for model, count in summary['requests_by_model'].items():
            pct = (count / summary['total_requests'] * 100) if summary['total_requests'] > 0 else 0
            print(f"   {model}: {count} requests ({pct:.1f}%)")
        
        print("=" * 70 + "\n")


def estimate_cost(model: str, tokens: int) -> float:
    """Estimar costo basado en modelo y tokens (precios aproximados)"""
    pricing = {
        "gpt-4o": {"input": 0.0025, "output": 0.010},  # per 1K tokens
        "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},
    }
    
    model_pricing = pricing.get(model, pricing["gpt-4o-mini"])
    # SimplificaciÃ³n: asumimos 50/50 input/output
    avg_price = (model_pricing["input"] + model_pricing["output"]) / 2
    return (tokens / 1000) * avg_price


def create_traced_chain(model: str):
    """Crear chain con tracing automÃ¡tico"""
    llm = ChatOpenAI(model=model, temperature=0.7)
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", "Eres un asistente Ãºtil y conciso."),
        ("user", "{question}")
    ])
    
    chain = {"question": RunnablePassthrough()} | prompt | llm
    
    return chain


def main():
    """DemostraciÃ³n de sistema de observabilidad"""
    print("=" * 70)
    print("Sistema de Tracing y Observabilidad para ProducciÃ³n")
    print("=" * 70)
    
    if not os.getenv("OPENAI_API_KEY"):
        raise ValueError("âŒ OPENAI_API_KEY no configurada")
    
    # Crear monitor
    monitor = ProductionMonitor()
    
    # Simular diferentes tipos de requests
    test_scenarios = [
        {"model": "gpt-4o-mini", "question": "Â¿QuÃ© es Python?"},
        {"model": "gpt-4o-mini", "question": "Dame 3 beneficios de usar Docker"},
        {"model": "gpt-4o", "question": "Explica en detalle cÃ³mo funcionan los transformers en NLP"},
        {"model": "gpt-4o-mini", "question": "Â¿CuÃ¡l es la capital de Francia?"},
        {"model": "gpt-4o-mini", "question": "Lista 5 lenguajes de programaciÃ³n"},
        {"model": "gpt-4o", "question": "DiseÃ±a una arquitectura de microservicios para e-commerce"},
    ]
    
    print(f"\nğŸš€ Ejecutando {len(test_scenarios)} requests simulados...\n")
    
    for i, scenario in enumerate(test_scenarios, 1):
        model = scenario["model"]
        question = scenario["question"]
        
        print(f"[{i}/{len(test_scenarios)}] {model}: {question[:50]}...")
        
        # Crear chain
        chain = create_traced_chain(model)
        
        # Medir latencia
        start_time = time.time()
        
        try:
            # Ejecutar (esto automÃ¡ticamente se registra en LangSmith si estÃ¡ configurado)
            result = chain.invoke(question)
            response = result.content
            
            latency_ms = (time.time() - start_time) * 1000
            
            # Estimar tokens (simplificado: contar caracteres / 4)
            estimated_tokens = (len(question) + len(response)) // 4
            
            # Estimar costo
            cost = estimate_cost(model, estimated_tokens)
            
            # Registrar mÃ©tricas
            monitor.record_request(model, estimated_tokens, latency_ms, cost)
            
            print(f"   âœ… Success - {latency_ms:.0f}ms, ~{estimated_tokens} tokens, ${cost:.5f}")
            
        except Exception as e:
            latency_ms = (time.time() - start_time) * 1000
            monitor.record_request(model, 0, latency_ms, 0, error=True)
            print(f"   âŒ Error: {str(e)}")
    
    # Mostrar dashboard
    monitor.print_dashboard()
    
    # Insights
    print("ğŸ’¡ INSIGHTS:")
    summary = monitor.get_summary()
    
    if summary['error_rate_percent'] > 5:
        print("   âš ï¸ Alta tasa de errores detectada - revisar logs")
    
    if summary['avg_latency_ms'] > 3000:
        print("   âš ï¸ Latencia promedio alta - considerar caching o modelo mÃ¡s rÃ¡pido")
    
    # Calcular savings potenciales
    gpt4_requests = summary['requests_by_model'].get('gpt-4o', 0)
    if gpt4_requests > 0:
        potential_savings = gpt4_requests * 0.01  # EstimaciÃ³n rough
        print(f"   ğŸ’° Potencial ahorro si se usa GPT-4o-mini: ~${potential_savings:.3f}")
    
    print("\nğŸ“ Nota: Todas las llamadas se registran en LangSmith si estÃ¡ configurado.")
    print("   Visita https://smith.langchain.com para ver traces detallados.")


if __name__ == "__main__":
    main()
