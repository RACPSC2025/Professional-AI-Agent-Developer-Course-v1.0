import numpy as np

def softmax(x):
    """Calcula la funci√≥n Softmax para un array numpy."""
    e_x = np.exp(x - np.max(x)) # Restamos max para estabilidad num√©rica
    return e_x / e_x.sum(axis=-1, keepdims=True)

def self_attention(query, key, value):
    """
    Implementaci√≥n manual de Scaled Dot-Product Attention.
    Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V
    """
    d_k = query.shape[-1]
    
    # 1. Producto Punto (Matmul) entre Query y Key Transpuesta
    scores = np.matmul(query, key.T)
    
    # 2. Escalamiento (Scaling)
    scaled_scores = scores / np.sqrt(d_k)
    
    # 3. Softmax para obtener probabilidades (pesos de atenci√≥n)
    attention_weights = softmax(scaled_scores)
    
    # 4. Multiplicaci√≥n por Value
    output = np.matmul(attention_weights, value)
    
    return output, attention_weights

def main():
    print("üßÆ SIMULACI√ìN DE SELF-ATTENTION CON NUMPY\n")
    
    # Simulamos embeddings para 3 palabras: "AI", "Agents", "Rules"
    # Dimensi√≥n del embedding = 4
    # En la realidad, estos n√∫meros se aprenden durante el entrenamiento.
    inputs = np.array([
        [1.0, 0.0, 1.0, 0.0], # AI
        [0.0, 1.0, 0.0, 1.0], # Agents
        [1.0, 1.0, 1.0, 1.0]  # Rules
    ])
    
    print("Input Embeddings (3 palabras, dim=4):")
    print(inputs)
    
    # En self-attention simple, Q, K, V suelen ser proyecciones del input.
    # Aqu√≠ usaremos el input directo para simplificar.
    Q = inputs
    K = inputs
    V = inputs
    
    print("\nCalculando Atenci√≥n...")
    output, weights = self_attention(Q, K, V)
    
    print("\n‚úÖ Pesos de Atenci√≥n (Attention Weights):")
    print("Muestra cu√°nto se enfoca cada palabra en las otras.")
    print(np.round(weights, 2))
    
    print("\n‚úÖ Salida (Contextualized Embeddings):")
    print("Nuevas representaciones que mezclan informaci√≥n basada en la atenci√≥n.")
    print(np.round(output, 2))

if __name__ == "__main__":
    main()
