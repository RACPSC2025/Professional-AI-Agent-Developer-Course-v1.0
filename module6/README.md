# M√≥dulo 6: IA Confiable y Evaluaci√≥n (Trustworthy AI)

## üéØ Objetivos del M√≥dulo
Es f√°cil hacer una demo que funcione el 80% de las veces. Lo dif√≠cil es llegar al 99%. En este m√≥dulo, aprender√°s a medir la calidad de tus agentes, protegerlos contra ataques y asegurar que no filtren datos sensibles.

## üìö Conceptos Clave

### 1. Evaluaci√≥n (Evals)
-   **LLM-as-a-Judge:** Usar un modelo potente (GPT-4) para evaluar las respuestas de un modelo m√°s peque√±o.
-   **M√©tricas RAG:** Context Recall, Context Precision, Faithfulness (¬øLa respuesta se basa en el contexto o alucina?).
-   **Herramientas:** Ragas, DeepEval, LangSmith.

### 2. Guardrails (Barandillas)
-   Capas de seguridad que interceptan la entrada del usuario o la salida del agente.
-   **NeMo Guardrails:** Definir flujos permitidos y bloqueados.
-   **Guardrails AI:** Validadores program√°ticos (no PII, no toxicidad, no jailbreak).

### 3. Adversarial Testing (Red Teaming)
-   **Prompt Injection:** Intentos de manipular el sistema prompt.
-   **Jailbreak:** Evadir restricciones de seguridad.
-   **Data Leakage:** Extraer informaci√≥n sensible del prompt.
-   **Automated Red Team:** Usar LLMs para generar ataques autom√°ticamente.

### 4. Bias y Fairness
-   Detectar sesgos en respuestas (g√©nero, raza, edad).
-   M√©tricas de equidad en sistemas de clasificaci√≥n.
-   Debiasing techniques.

## üíª Snippet de C√≥digo: Evaluaci√≥n con Ragas

```python
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_recall

# Dataset de prueba
data = {
    'question': ['¬øC√≥mo reseteo mi password?'],
    'answer': ['Ve a configuraci√≥n y pulsa reset.'],
    'contexts': [['Para resetear password, ir a settings...']]
}

# Evaluar
results = evaluate(
    dataset=data,
    metrics=[faithfulness, answer_relevancy, context_recall]
)

print(results)
# {'faithfulness': 0.92, 'answer_relevancy': 0.85, 'context_recall': 0.78}
```

## üõ†Ô∏è Proyectos Pr√°cticos

### üü¢ Nivel B√°sico: Sistema de Evaluaci√≥n RAG
**Archivo:** `01_rag_evaluation_system.py`
-   **Concepto:** Evaluar calidad de respuestas RAG autom√°ticamente.
-   **Framework:** LangChain + Ragas
-   **Caso de uso:** CI/CD pipeline que valida calidad antes de deploy.

### üü° Nivel Intermedio: Implementaci√≥n de Guardrails
**Archivo:** `02_guardrails_implementation.py`
-   **Concepto:** Validadores de entrada/salida para protecci√≥n.
-   **Framework:** Guardrails AI + LangChain
-   **Caso de uso:** Chatbot corporativo que bloquea informaci√≥n sensible.

### üî¥ Nivel Avanzado: Framework de Red Teaming
**Archivo:** `03_advanced_redteam_framework.py`
-   **Concepto:** Sistema automatizado de adversarial testing.
-   **Framework:** LangChain con evaluadores personalizados
-   **Caso de uso:** Auditor√≠a de seguridad de agentes empresariales.

## üéì Mejores Pr√°cticas

1. **Evaluar continuamente:** No solo al principio, tambi√©n en producci√≥n.
2. **Usar m√∫ltiples m√©tricas:** No hay una m√©trica perfecta.
3. **Test adversarial:** Asume que usuarios maliciosos intentar√°n romper tu sistema.
4. **Logging completo:** Registra todas las interacciones para an√°lisis post-mortem.
5. **Human-in-the-loop:** Para decisiones cr√≠ticas, siempre tener revisi√≥n humana.
6. **Versioning de prompts:** Trackear cambios en prompts como c√≥digo.

