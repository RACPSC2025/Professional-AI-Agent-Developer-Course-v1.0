# M√≥dulo 9: Metacognici√≥n y Auto-Evoluci√≥n

![Module 9 Banner](../images/module9_banner.png)

![Level](https://img.shields.io/badge/Nivel-Experto-F39C12?style=for-the-badge&logo=expert&logoColor=white)
![Time](https://img.shields.io/badge/Tiempo-6_Horas-A7C7E7?style=for-the-badge&labelColor=2D2D44)
![Stack](https://img.shields.io/badge/Stack-Reflexion_|_Self--RAG_|_DSPy-9B59B6?style=for-the-badge)

> *"La verdadera inteligencia no es saberlo todo, sino saber qu√© hacer cuando no sabes qu√© hacer."* ‚Äî Jean Piaget

---

## üéØ Objetivos del M√≥dulo

Hasta ahora, nuestros agentes han sido "inteligentes" pero est√°ticos. Si cometen un error, lo repiten. Si el prompt no es perfecto, fallan.
En este m√≥dulo, cruzamos la frontera hacia la **Metacognici√≥n**: agentes que piensan sobre su propio pensamiento, critican su trabajo y mejoran sus propios prompts autom√°ticamente.

Aprender√°s:
- üß† **Reflexion:** C√≥mo implementar bucles de auto-correcci√≥n verbal.
- üìö **Self-RAG:** Agentes que deciden *cu√°ndo* buscar informaci√≥n y critican lo que encuentran.
- üß¨ **DSPy (Auto-Evoluci√≥n):** Dejar de escribir prompts manuales y dejar que el agente los optimice matem√°ticamente.

---

## üìö √çndice

1. [¬øQu√© es la Metacognici√≥n en IA?](#1-qu√©-es-la-metacognici√≥n-en-ia)
2. [Reflexion: El Bucle de Auto-Mejora](#2-reflexion-el-bucle-de-auto-mejora)
3. [Self-RAG: RAG con Criterio](#3-self-rag-rag-con-criterio)
4. [DSPy: Programaci√≥n de Prompts](#4-dspy-programaci√≥n-de-prompts)
5. [Proyectos Pr√°cticos](#-proyectos-pr√°cticos)

---

## 1. ¬øQu√© es la Metacognici√≥n en IA?

La metacognici√≥n es la capacidad de "pensar sobre el pensamiento". En LLMs, esto se traduce en tres capacidades cr√≠ticas:

1.  **Self-Monitoring:** "¬øEstoy progresando hacia la soluci√≥n?"
2.  **Self-Correction:** "Comet√≠ un error de sintaxis, debo corregirlo."
3.  **Self-Reflection:** "¬øPor qu√© fall√© antes? Ah, olvid√© importar la librer√≠a."

A diferencia del **Chain-of-Thought (CoT)** que es un razonamiento lineal, la metacognici√≥n es un **bucle de retroalimentaci√≥n**.

---

## 2. Reflexion: El Bucle de Auto-Mejora

El paper *"Reflexion: Language Agents with Verbal Reinforcement Learning"* (Shinn et al., 2023) introdujo una idea revolucionaria: en lugar de actualizar los pesos del modelo (caro), actualizamos su memoria verbal (barato).

### Arquitectura Reflexion

```mermaid
graph TD
    Task[Tarea] --> Actor
    Actor -->|Intento 1| Evaluator{Evaluator}
    Evaluator -->|‚ùå Fallo| SelfReflection[Self-Reflection]
    SelfReflection -->|Genera Feedback| Memory[(Memoria Epis√≥dica)]
    Memory -->|Contexto Mejorado| Actor
    Actor -->|Intento 2| Evaluator
    Evaluator -->|‚úÖ √âxito| Success[Fin]
    
    style Actor fill:#4A90E2,color:#fff
    style SelfReflection fill:#9B59B6,color:#fff
    style Evaluator fill:#E74C3C,color:#fff
```

El agente no solo reintenta, sino que **aprende** de su intento fallido mediante una "lecci√≥n" escrita que se inyecta en el siguiente prompt.

---

## 3. Self-RAG: RAG con Criterio

El RAG tradicional es ciego: siempre busca, siempre conf√≠a en lo que encuentra. **Self-RAG** (Self-Reflective RAG) introduce tokens especiales para criticar cada paso.

### El Flujo de Decisi√≥n

1.  **Retrieve?** -> ¬øNecesito buscar info externa o ya lo s√©?
2.  **IsRel?** -> ¬øLo que encontr√© es relevante para la pregunta?
3.  **IsSup?** -> ¬øMi respuesta est√° soportada por la evidencia?
4.  **IsUse?** -> ¬øEs √∫til la respuesta final?

Si alguna m√©trica falla, el agente puede decidir buscar de nuevo o reescribir la respuesta.

---

## 4. DSPy: Programaci√≥n de Prompts

Escribir prompts a mano ("Act√∫a como un experto...") es fr√°gil y dif√≠cil de escalar. **DSPy** (Declarative Self-improving Python) cambia el paradigma:

- **T√∫ defines:** La firma (Input -> Output) y la m√©trica de √©xito.
- **DSPy define:** El prompt exacto y los ejemplos few-shot.

El "Optimizador" de DSPy (Teleprompter) prueba miles de variaciones de prompts y ejemplos hasta encontrar la combinaci√≥n que maximiza tu m√©trica. ¬°Es como un compilador para LLMs!

```python
# En lugar de escribir un prompt largo:
class RAG(dspy.Module):
    def forward(self, question):
        context = self.retrieve(question)
        return self.generate_answer(context, question)

# DSPy optimiza c√≥mo pedirle al modelo que haga esto.
```

---

## üõ†Ô∏è Proyectos Pr√°cticos

### üß† Proyecto 1: Agente Reflexion (LangGraph)
**Archivo:** [`01_reflexion_agent.py`](01_reflexion_agent.py)
- **Objetivo:** Un agente que escribe c√≥digo Python.
- **Mec√°nica:** Escribe c√≥digo -> Lo ejecuta -> Si falla, lee el error -> Reflexiona -> Reescribe.
- **Resultado:** C√≥digo robusto que se arregla solo.

### üìö Proyecto 2: Self-RAG Minimalista
**Archivo:** [`02_self_rag_minimal.py`](02_self_rag_minimal.py)
- **Objetivo:** Sistema de Q&A que no alucina.
- **Mec√°nica:** Genera respuesta y luego se auto-critica ("¬øInvent√© esto?"). Si detecta alucinaci√≥n, corrige.

### üß¨ Proyecto 3: Auto-Optimizaci√≥n con DSPy
**Archivo:** [`03_dspy_auto_optimizer.py`](03_dspy_auto_optimizer.py)
- **Objetivo:** Crear un clasificador de sentimientos perfecto.
- **Mec√°nica:** Empezamos con cero ejemplos. DSPy "compila" el programa y encuentra los mejores prompts y ejemplos few-shot autom√°ticamente.

---

## üéì Referencias

- **Paper Reflexion:** [arxiv.org/abs/2303.11366](https://arxiv.org/abs/2303.11366)
- **Paper Self-RAG:** [arxiv.org/abs/2310.11511](https://arxiv.org/abs/2310.11511)
- **DSPy Repo:** [github.com/stanfordnlp/dspy](https://github.com/stanfordnlp/dspy)

---

<div align="center">

**[‚¨ÖÔ∏è M√≥dulo Anterior](../module8/README.md)** | **[üè† Inicio](../README.md)** | **[Siguiente M√≥dulo ‚û°Ô∏è](../module10/README.md)**

</div>
