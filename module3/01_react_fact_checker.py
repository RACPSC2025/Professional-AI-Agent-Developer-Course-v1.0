"""
üü¢ NIVEL B√ÅSICO: VERIFICADOR DE HECHOS (PATR√ìN REACT)
-----------------------------------------------------
Este script implementa el patr√≥n ReAct (Reason + Act) DESDE CERO.
No usamos LangChain ni AutoGen aqu√≠ para que entiendas la l√≥gica interna.

El ciclo es:
1. PENSAMIENTO: El LLM analiza qu√© necesita saber.
2. ACCI√ìN: El LLM elige una herramienta (ej: buscar en Wikipedia).
3. OBSERVACI√ìN: El c√≥digo ejecuta la herramienta y le da el resultado al LLM.
4. REPETIR: Hasta que el LLM tenga suficiente info para responder.

Caso de Uso: Verificar afirmaciones complejas que requieren m√∫ltiples pasos.
"""

import os
import re
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_community.tools import DuckDuckGoSearchRun

load_dotenv()

# --- 1. HERRAMIENTAS ---
search_tool = DuckDuckGoSearchRun()

def execute_tool(tool_name, tool_input):
    if tool_name == "SEARCH":
        print(f"   üîç BUSCANDO: {tool_input}...")
        try:
            return search_tool.run(tool_input)
        except Exception as e:
            return f"Error en b√∫squeda: {e}"
    return "Herramienta no encontrada."

# --- 2. PROMPT DEL SISTEMA (EL CEREBRO REACT) ---
REACT_SYSTEM_PROMPT = """
Eres un Verificador de Hechos experto. Tu trabajo es validar afirmaciones.
Para responder, DEBES usar el siguiente formato:

Pensamiento: [Tu razonamiento sobre qu√© hacer ahora]
Acci√≥n: [SEARCH]
Entrada de Acci√≥n: [Lo que quieres buscar]

Cuando tengas la respuesta final:
Pensamiento: [Ya tengo la respuesta]
Respuesta Final: [Tu conclusi√≥n veraz]

Ejemplo:
Pregunta: ¬øQui√©n es el CEO de Apple?
Pensamiento: Debo buscar qui√©n es el CEO actual.
Acci√≥n: SEARCH
Entrada de Acci√≥n: CEO actual de Apple
... (El sistema te dar√° la Observaci√≥n) ...
Pensamiento: La b√∫squeda dice que es Tim Cook.
Respuesta Final: El CEO de Apple es Tim Cook.

¬°EMPIEZA!
"""

# --- 3. EL BUCLE REACT (EL MOTOR) ---
def run_react_agent(question, max_steps=5):
    print(f"ü§ñ PREGUNTA: {question}\n")
    
    llm = ChatOpenAI(model="gpt-4o", temperature=0)
    history = [
        ("system", REACT_SYSTEM_PROMPT),
        ("user", f"Pregunta: {question}")
    ]
    
    step = 0
    while step < max_steps:
        step += 1
        print(f"--- PASO {step} ---")
        
        # 1. LLM Genera Pensamiento + Acci√≥n
        response = llm.invoke(history).content
        print(f"üß† AGENTE:\n{response}")
        history.append(("assistant", response))
        
        # 2. Detectar si hay Respuesta Final
        if "Respuesta Final:" in response:
            return response.split("Respuesta Final:")[1].strip()
            
        # 3. Parsear Acci√≥n (Regex simple)
        # Buscamos: Acci√≥n: SEARCH \n Entrada de Acci√≥n: query
        action_match = re.search(r"Acci√≥n:\s*(\w+)", response)
        input_match = re.search(r"Entrada de Acci√≥n:\s*(.+)", response)
        
        if action_match and input_match:
            tool_name = action_match.group(1)
            tool_input = input_match.group(1).strip()
            
            # 4. Ejecutar Herramienta
            observation = execute_tool(tool_name, tool_input)
            print(f"üëÄ OBSERVACI√ìN: {observation[:200]}...") # Truncamos para no ensuciar log
            
            # 5. Alimentar de vuelta al LLM
            history.append(("user", f"Observaci√≥n: {observation}"))
        else:
            print("‚ö†Ô∏è El agente no gener√≥ una acci√≥n v√°lida. Forzando continuaci√≥n...")
            history.append(("user", "Por favor sigue el formato: Acci√≥n: [TOOL] / Entrada de Acci√≥n: [INPUT]"))

    return "‚ùå Se alcanz√≥ el l√≠mite de pasos sin respuesta."

# --- 4. EJECUCI√ìN ---
if __name__ == "__main__":
    # Pregunta trampa: Requiere saber qui√©n invent√≥ el transistor Y si gan√≥ 2 Nobels
    q = "¬øEs verdad que el inventor del transistor gan√≥ dos premios Nobel?"
    resultado = run_react_agent(q)
    print(f"\n‚úÖ RESULTADO FINAL: {resultado}")
