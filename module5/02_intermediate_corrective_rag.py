"""
MÃ³dulo 5 - Ejemplo Intermedio: Corrective RAG (CRAG)
Framework: LangGraph
Caso de uso: Sistema de soporte tÃ©cnico que auto-corrige bÃºsquedas irrelevantes

Corrective RAG evalÃºa la calidad de los documentos recuperados y, si no son relevantes,
busca en fuentes externas (web search) o reformula la consulta.

InstalaciÃ³n:
pip install langgraph langchain langchain-openai langchain-community chromadb tavily-python
"""

import os
from typing import TypedDict, List
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langgraph.graph import StateGraph, END
from dotenv import load_dotenv

load_dotenv()

# ConfiguraciÃ³n
LLM = ChatOpenAI(model="gpt-4o-mini", temperature=0)
EMBEDDINGS = OpenAIEmbeddings()


class CRAGState(TypedDict):
    """Estado del grafo de Corrective RAG"""
    question: str
    documents: List[Document]
    generation: str
    relevance_score: str  # "relevant" o "irrelevant"
    web_search_needed: bool


def create_knowledge_base() -> Chroma:
    """Crear base de conocimientos de soporte tÃ©cnico"""
    docs = [
        "Para resetear tu password, ve a ConfiguraciÃ³n > Seguridad > Cambiar Password. "
        "NecesitarÃ¡s tu email de verificaciÃ³n.",
        
        "El error 'Connection Timeout' generalmente indica problemas de red. "
        "Verifica tu firewall y asegÃºrate de que el puerto 443 estÃ© abierto.",
        
        "Para exportar tus datos, usa el botÃ³n 'Exportar' en el panel principal. "
        "Soportamos formatos CSV, JSON y Excel.",
        
        "Si la aplicaciÃ³n se cierra inesperadamente, revisa los logs en C:\\AppLogs. "
        "Busca mensajes con nivel ERROR o FATAL.",
        
        "Para actualizar a la versiÃ³n premium, ve a Cuenta > SuscripciÃ³n. "
        "Aceptamos tarjetas de crÃ©dito y PayPal."
    ]
    
    # Dividir en chunks
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)
    documents = [Document(page_content=doc) for doc in docs]
    splits = text_splitter.split_documents(documents)
    
    # Crear vectorstore
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=EMBEDDINGS,
        collection_name="tech_support"
    )
    return vectorstore


def retrieve_documents(state: CRAGState) -> CRAGState:
    """Paso 1: Recuperar documentos del vectorstore"""
    print(f"\nğŸ“š Recuperando documentos para: '{state['question']}'")
    
    vectorstore = create_knowledge_base()
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    
    documents = retriever.get_relevant_documents(state["question"])
    
    print(f"âœ… Recuperados {len(documents)} documentos")
    for i, doc in enumerate(documents, 1):
        print(f"  {i}. {doc.page_content[:80]}...")
    
    return {**state, "documents": documents}


def grade_documents(state: CRAGState) -> CRAGState:
    """Paso 2: Evaluar relevancia de los documentos (LLM-as-a-Judge)"""
    print("\nâš–ï¸ Evaluando relevancia de documentos...")
    
    # Concatenar contenido de documentos
    docs_content = "\n\n".join([doc.page_content for doc in state["documents"]])
    
    # Prompt para evaluaciÃ³n
    grading_prompt = f"""Eres un evaluador de relevancia. Analiza si los siguientes documentos 
son relevantes para responder la pregunta del usuario.

Pregunta: {state['question']}

Documentos:
{docs_content}

Â¿Los documentos contienen informaciÃ³n relevante para responder la pregunta?
Responde SOLO con 'relevant' o 'irrelevant'."""
    
    response = LLM.invoke(grading_prompt)
    relevance = response.content.strip().lower()
    
    if relevance == "relevant":
        print("âœ… Documentos RELEVANTES - Proceder a generar respuesta")
        return {**state, "relevance_score": "relevant", "web_search_needed": False}
    else:
        print("âŒ Documentos IRRELEVANTES - Se requiere bÃºsqueda web")
        return {**state, "relevance_score": "irrelevant", "web_search_needed": True}


def web_search(state: CRAGState) -> CRAGState:
    """Paso 3a: BÃºsqueda web si los documentos no son relevantes"""
    print("\nğŸŒ Realizando bÃºsqueda web complementaria...")
    
    # Tavily para bÃºsqueda web
    web_search_tool = TavilySearchResults(max_results=3)
    search_results = web_search_tool.invoke({"query": state["question"]})
    
    # Convertir resultados a Documents
    web_docs = [
        Document(
            page_content=result.get("content", ""),
            metadata={"source": result.get("url", "unknown")}
        )
        for result in search_results
    ]
    
    print(f"âœ… Encontrados {len(web_docs)} resultados web")
    
    # Combinar con documentos originales
    all_documents = state["documents"] + web_docs
    
    return {**state, "documents": all_documents, "relevance_score": "relevant"}


def generate_answer(state: CRAGState) -> CRAGState:
    """Paso 3b/4: Generar respuesta usando documentos relevantes"""
    print("\nğŸ¤– Generando respuesta final...")
    
    # Context from documents
    context = "\n\n".join([doc.page_content for doc in state["documents"]])
    
    generation_prompt = f"""Eres un asistente de soporte tÃ©cnico. Responde la pregunta del usuario 
basÃ¡ndote ÃšNICAMENTE en el contexto proporcionado.

Contexto:
{context}

Pregunta: {state['question']}

Respuesta (si no hay informaciÃ³n suficiente, dilo claramente):"""
    
    response = LLM.invoke(generation_prompt)
    answer = response.content.strip()
    
    print(f"\nâœ… Respuesta generada:\n{answer}")
    
    return {**state, "generation": answer}


def should_web_search(state: CRAGState) -> str:
    """DecisiÃ³n: Â¿Necesitamos bÃºsqueda web?"""
    if state.get("web_search_needed", False):
        return "web_search"
    else:
        return "generate"


# ConstrucciÃ³n del grafo de Corrective RAG
def create_crag_graph():
    """Construir el grafo de flujo de Corrective RAG"""
    workflow = StateGraph(CRAGState)
    
    # Nodos
    workflow.add_node("retrieve", retrieve_documents)
    workflow.add_node("grade", grade_documents)
    workflow.add_node("web_search", web_search)
    workflow.add_node("generate", generate_answer)
    
    # Flujo
    workflow.set_entry_point("retrieve")
    workflow.add_edge("retrieve", "grade")
    
    # DecisiÃ³n condicional: Â¿web search o generar directamente?
    workflow.add_conditional_edges(
        "grade",
        should_web_search,
        {
            "web_search": "web_search",
            "generate": "generate"
        }
    )
    
    workflow.add_edge("web_search", "generate")
    workflow.add_edge("generate", END)
    
    return workflow.compile()


def main():
    """FunciÃ³n principal para demostrar Corrective RAG"""
    print("=" * 70)
    print("Sistema de Soporte TÃ©cnico con Corrective RAG")
    print("=" * 70)
    
    # Compilar grafo
    app = create_crag_graph()
    
    # Test cases
    test_questions = [
        "Â¿CÃ³mo reseteo mi password?",  # Debe ser respondida con docs locales
        "Â¿CuÃ¡les son las Ãºltimas noticias de OpenAI?",  # Requiere web search
        "Â¿CÃ³mo exporto mis datos?"  # Debe ser respondida con docs locales
    ]
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n{'=' * 70}")
        print(f"PREGUNTA {i}: {question}")
        print("=" * 70)
        
        # Ejecutar grafo
        initial_state = CRAGState(
            question=question,
            documents=[],
            generation="",
            relevance_score="",
            web_search_needed=False
        )
        
        result = app.invoke(initial_state)
        
        print(f"\nğŸ“ RESPUESTA FINAL:")
        print(f"{result['generation']}")
        print(f"\nğŸ” Fuentes usadas: {'Internal KB + Web' if result['web_search_needed'] else 'Internal KB only'}")
        print("=" * 70)


if __name__ == "__main__":
    # Verificar variables de entorno
    if not os.getenv("OPENAI_API_KEY"):
        raise ValueError("âŒ OPENAI_API_KEY no configurada")
    if not os.getenv("TAVILY_API_KEY"):
        raise ValueError("âŒ TAVILY_API_KEY no configurada")
    
    main()
