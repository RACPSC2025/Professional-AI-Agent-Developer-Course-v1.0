# Parte 2: Text Splitters - El Arte del Chunking Inteligente

![Text Splitters](https://img.shields.io/badge/RAG_Pipeline-Text_Splitters-7B68EE?style=for-the-badge)

## üìñ √çndice
1. [Fundamentos Conceptuales](#fundamentos-conceptuales)
2. [El Dilema del Chunking](#el-dilema-del-chunking)
3. [Estrategias de Chunking](#estrategias-de-chunking)
4. [Implementaci√≥n con M√∫ltiples Frameworks](#implementaci√≥n-con-m√∫ltiples-frameworks)
5. [T√©cnicas Avanzadas](#t√©cnicas-avanzadas)
6. [Mejores Pr√°cticas](#mejores-pr√°cticas)

---

## üéØ Fundamentos Conceptuales

### ¬øPor Qu√© Necesitamos Text Splitters?

Los modelos de lenguaje tienen **l√≠mites de contexto**. No podemos enviar un documento completo de 100 p√°ginas como contexto. Adem√°s, documentos grandes contienen informaci√≥n irrelevante que **diluye la precisi√≥n** del retrieval.

```mermaid
graph TD
    A[Documento Completo<br/>10,000 palabras] --> B{Text Splitter}
    B --> C[Chunk 1<br/>200 palabras]
    B --> D[Chunk 2<br/>200 palabras]
    B --> E[Chunk 3<br/>200 palabras]
    B --> F[...]
    B --> G[Chunk N<br/>200 palabras]
    
    C --> H[Embedding 1]
    D --> I[Embedding 2]
    E --> J[Embedding 3]
    
    style B fill:#7B68EE,stroke:#4B0082,stroke-width:3px,color:#fff
```

### El Problema Fundamental

> [!IMPORTANT]
> **El chunking es el factor m√°s cr√≠tico en la calidad de un sistema RAG**. Un chunking mal dise√±ado puede:
> - **Fragmentar contexto**: Dividir informaci√≥n relacionada
> - **Perder coherencia**: Chunks sin sentido completo
> - **Degradar retrieval**: Embeddings de baja calidad
> - **Aumentar costos**: M√°s chunks = m√°s embeddings = m√°s dinero

---

## ‚öñÔ∏è El Dilema del Chunking

### El Trade-off Fundamental

| Aspecto | Chunks Peque√±os (100-200 tokens) | Chunks Grandes (800-1000 tokens) |
|---------|----------------------------------|----------------------------------|
| **Precisi√≥n** | ‚úÖ Alta - informaci√≥n espec√≠fica | ‚ùå Baja - informaci√≥n diluida |
| **Contexto** | ‚ùå Bajo - puede perder relaciones | ‚úÖ Alto - contexto completo |
| **Costo** | ‚ùå Alto - m√°s embeddings | ‚úÖ Bajo - menos embeddings |
| **Velocidad** | ‚ùå Lenta - m√°s b√∫squedas | ‚úÖ R√°pida - menos b√∫squedas |
| **Relevancia** | ‚úÖ Alta - matches exactos | ‚ùå Baja - matches gen√©ricos |

### Visualizaci√≥n del Impacto

```python
# Ejemplo: Documento sobre Python

# ‚ùå Chunk demasiado peque√±o (pierde contexto)
chunk_1 = "Las listas en Python son mutables"
# Pregunta: "¬øC√≥mo modificar una lista?"
# Problema: No hay ejemplos de c√≥digo

# ‚ùå Chunk demasiado grande (informaci√≥n diluida)
chunk_2 = """
Python es un lenguaje de programaci√≥n interpretado...
Las listas son estructuras de datos mutables...
Los diccionarios almacenan pares clave-valor...
Las funciones se definen con def...
[500 palabras m√°s sobre diversos temas]
"""
# Problema: Demasiados temas, embedding poco espec√≠fico

# ‚úÖ Chunk √≥ptimo (balance perfecto)
chunk_3 = """
Las listas en Python son mutables, lo que significa que puedes 
modificar sus elementos despu√©s de crearlas.

Ejemplos de modificaci√≥n:
- Cambiar un elemento: lista[0] = 'nuevo_valor'
- A√±adir elementos: lista.append('item')
- Eliminar elementos: lista.remove('item')

Esto las diferencia de las tuplas, que son inmutables.
"""
# ‚úÖ Contexto completo + ejemplos + informaci√≥n relacionada
```

---

## üîß Estrategias de Chunking

### 1. **Character-Based Splitting** (B√°sico)

Divisi√≥n simple por n√∫mero de caracteres.

```python
from langchain.text_splitter import CharacterTextSplitter

# Configuraci√≥n b√°sica
text_splitter = CharacterTextSplitter(
    separator="\n\n",        # Dividir por p√°rrafos
    chunk_size=1000,         # M√°ximo 1000 caracteres
    chunk_overlap=200,       # 200 caracteres de overlap
    length_function=len,     # Funci√≥n para medir longitud
)

# Uso
text = """
P√°rrafo 1: Introducci√≥n a RAG...

P√°rrafo 2: Document Loaders...

P√°rrafo 3: Text Splitters...
"""

chunks = text_splitter.split_text(text)
print(f"Chunks creados: {len(chunks)}")
```

**Ventajas**: Simple, r√°pido  
**Desventajas**: No respeta l√≠mites sem√°nticos

### 2. **Recursive Character Splitting** (Recomendado)

Divisi√≥n inteligente que respeta jerarqu√≠a de separadores.

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Configuraci√≥n profesional
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=200,
    length_function=len,
    separators=[
        "\n\n",    # Primero intenta dividir por p√°rrafos
        "\n",      # Luego por l√≠neas
        ". ",      # Luego por oraciones
        " ",       # Finalmente por palabras
        ""         # √öltimo recurso: caracteres
    ]
)

# Uso con documentos
from langchain.schema import Document

documents = [
    Document(
        page_content="Contenido largo del documento...",
        metadata={"source": "doc1.pdf", "page": 1}
    )
]

# Split preservando metadata
chunks = text_splitter.split_documents(documents)

for i, chunk in enumerate(chunks):
    print(f"Chunk {i}:")
    print(f"  Contenido: {chunk.page_content[:100]}...")
    print(f"  Metadata: {chunk.metadata}")
```

**Ventajas**: Respeta estructura del texto  
**Desventajas**: A√∫n no considera sem√°ntica

### 3. **Token-Based Splitting** (Para LLMs)

Divisi√≥n por tokens (unidades que entiende el LLM).

```python
from langchain.text_splitter import TokenTextSplitter

# Usar tiktoken (tokenizer de OpenAI)
text_splitter = TokenTextSplitter(
    encoding_name="cl100k_base",  # Encoding de GPT-4
    chunk_size=400,                # 400 tokens por chunk
    chunk_overlap=50               # 50 tokens de overlap
)

text = "Tu texto largo aqu√≠..."
chunks = text_splitter.split_text(text)

# Verificar tama√±o real en tokens
import tiktoken
encoder = tiktoken.get_encoding("cl100k_base")

for i, chunk in enumerate(chunks):
    token_count = len(encoder.encode(chunk))
    print(f"Chunk {i}: {token_count} tokens")
```

**Ventajas**: Precisi√≥n exacta para l√≠mites de LLM  
**Desventajas**: M√°s lento que character-based

### 4. **Semantic Chunking** (Avanzado)

Divisi√≥n basada en **similitud sem√°ntica** entre oraciones.

```python
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings

# Configurar semantic chunker
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

text_splitter = SemanticChunker(
    embeddings=embeddings,
    breakpoint_threshold_type="percentile",  # "percentile" | "standard_deviation" | "interquartile"
    breakpoint_threshold_amount=95           # Percentil 95
)

# Uso
text = """
Python es un lenguaje de programaci√≥n. Es muy popular para ciencia de datos.

Los pandas son animales nativos de China. Comen principalmente bamb√∫.

El framework pandas de Python es excelente para an√°lisis de datos.
"""

chunks = text_splitter.split_text(text)

# Resultado: Agrupa oraciones sem√°nticamente relacionadas
# Chunk 1: Python + ciencia de datos
# Chunk 2: Pandas animales
# Chunk 3: Pandas framework
```

**C√≥mo funciona**:
1. Calcula embeddings de cada oraci√≥n
2. Mide similitud coseno entre oraciones consecutivas
3. Divide donde la similitud cae por debajo del threshold

**Ventajas**: Preserva coherencia sem√°ntica  
**Desventajas**: M√°s costoso (requiere embeddings)

---

## üíª Implementaci√≥n con M√∫ltiples Frameworks

### Ejemplo 1: LangChain - Chunking B√°sico a Avanzado

```python
"""
Ejemplo B√°sico: Comparaci√≥n de Estrategias de Chunking
Framework: LangChain
Objetivo: Demostrar diferencias entre m√©todos de splitting
"""

from langchain.text_splitter import (
    CharacterTextSplitter,
    RecursiveCharacterTextSplitter,
    TokenTextSplitter
)
from langchain_openai import OpenAIEmbeddings
from langchain_experimental.text_splitter import SemanticChunker


# Texto de ejemplo
SAMPLE_TEXT = """
Retrieval-Augmented Generation (RAG) es una t√©cnica que combina 
recuperaci√≥n de informaci√≥n con generaci√≥n de lenguaje natural.

El proceso RAG consta de varios pasos fundamentales:

1. Document Loading: Cargar documentos de diversas fuentes.
2. Text Splitting: Dividir documentos en chunks manejables.
3. Embedding: Convertir chunks en vectores num√©ricos.
4. Vector Storage: Almacenar embeddings en una base de datos vectorial.
5. Retrieval: Recuperar chunks relevantes para una consulta.
6. Generation: Generar respuesta usando chunks como contexto.

Cada paso es cr√≠tico para el √©xito del sistema RAG.
"""


def compare_splitting_strategies():
    """Compara diferentes estrategias de splitting"""
    
    strategies = {
        "Character-Based": CharacterTextSplitter(
            separator="\n\n",
            chunk_size=200,
            chunk_overlap=50
        ),
        "Recursive": RecursiveCharacterTextSplitter(
            chunk_size=200,
            chunk_overlap=50,
            separators=["\n\n", "\n", ". ", " ", ""]
        ),
        "Token-Based": TokenTextSplitter(
            chunk_size=100,
            chunk_overlap=20
        ),
    }
    
    print("=" * 80)
    print("COMPARACI√ìN DE ESTRATEGIAS DE CHUNKING")
    print("=" * 80)
    
    for name, splitter in strategies.items():
        chunks = splitter.split_text(SAMPLE_TEXT)
        
        print(f"\n{'‚îÄ' * 80}")
        print(f"üìä Estrategia: {name}")
        print(f"{'‚îÄ' * 80}")
        print(f"Total de chunks: {len(chunks)}\n")
        
        for i, chunk in enumerate(chunks, 1):
            print(f"Chunk {i} ({len(chunk)} chars):")
            print(f"  {chunk[:100]}...")
            print()


def semantic_chunking_example():
    """Ejemplo de semantic chunking"""
    
    print("\n" + "=" * 80)
    print("SEMANTIC CHUNKING (Avanzado)")
    print("=" * 80 + "\n")
    
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    semantic_splitter = SemanticChunker(
        embeddings=embeddings,
        breakpoint_threshold_type="percentile",
        breakpoint_threshold_amount=90
    )
    
    chunks = semantic_splitter.split_text(SAMPLE_TEXT)
    
    print(f"Total de chunks sem√°nticos: {len(chunks)}\n")
    
    for i, chunk in enumerate(chunks, 1):
        print(f"Chunk Sem√°ntico {i}:")
        print(f"  {chunk}")
        print()


if __name__ == "__main__":
    # Comparar estrategias b√°sicas
    compare_splitting_strategies()
    
    # Demostrar semantic chunking
    # semantic_chunking_example()  # Descomentar si tienes API key de OpenAI
```

### Ejemplo 2: LlamaIndex - Chunking con Node Parser

```python
"""
Ejemplo Intermedio: Chunking con LlamaIndex
Framework: LlamaIndex
Objetivo: Usar Node Parsers para chunking avanzado
"""

from llama_index.core import Document
from llama_index.core.node_parser import (
    SentenceSplitter,
    SemanticSplitterNodeParser,
    TokenTextSplitter as LlamaTokenSplitter
)
from llama_index.embeddings.openai import OpenAIEmbedding


def llamaindex_sentence_splitter():
    """Sentence Splitter de LlamaIndex"""
    
    # Crear documento
    doc = Document(
        text="""
        RAG systems require careful chunking strategies. The quality of 
        your chunks directly impacts retrieval accuracy.
        
        Best practices include: maintaining semantic coherence, using 
        appropriate overlap, and considering token limits.
        
        Different use cases require different chunking approaches.
        """,
        metadata={"source": "rag_guide.md", "section": "chunking"}
    )
    
    # Configurar splitter
    splitter = SentenceSplitter(
        chunk_size=1024,
        chunk_overlap=200,
        separator=" "
    )
    
    # Crear nodes (equivalente a chunks en LangChain)
    nodes = splitter.get_nodes_from_documents([doc])
    
    print("=" * 80)
    print("LLAMAINDEX - SENTENCE SPLITTER")
    print("=" * 80 + "\n")
    
    for i, node in enumerate(nodes, 1):
        print(f"Node {i}:")
        print(f"  Text: {node.text[:100]}...")
        print(f"  Metadata: {node.metadata}")
        print(f"  Node ID: {node.node_id}")
        print()


def llamaindex_semantic_splitter():
    """Semantic Splitter de LlamaIndex"""
    
    doc = Document(
        text="""
        Machine learning models require large datasets. Training data 
        quality is crucial for model performance.
        
        Cats are popular pets. They are known for their independence.
        
        Deep learning is a subset of machine learning. It uses neural 
        networks with multiple layers.
        """,
        metadata={"source": "ml_guide.md"}
    )
    
    # Configurar embedding model
    embed_model = OpenAIEmbedding(model="text-embedding-3-small")
    
    # Semantic splitter
    splitter = SemanticSplitterNodeParser(
        buffer_size=1,
        breakpoint_percentile_threshold=95,
        embed_model=embed_model
    )
    
    nodes = splitter.get_nodes_from_documents([doc])
    
    print("=" * 80)
    print("LLAMAINDEX - SEMANTIC SPLITTER")
    print("=" * 80 + "\n")
    
    for i, node in enumerate(nodes, 1):
        print(f"Semantic Node {i}:")
        print(f"  {node.text}")
        print()


if __name__ == "__main__":
    llamaindex_sentence_splitter()
    # llamaindex_semantic_splitter()  # Requiere API key
```

### Ejemplo 3: Chunking Adaptativo Multi-Framework

```python
"""
Ejemplo Avanzado: Sistema de Chunking Adaptativo
Frameworks: LangChain + Custom Logic
Objetivo: Seleccionar estrategia de chunking seg√∫n tipo de contenido
"""

from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    Language,
    MarkdownHeaderTextSplitter
)
from langchain.schema import Document
from typing import List, Literal
import re


class AdaptiveChunker:
    """
    Chunker adaptativo que selecciona la estrategia √≥ptima
    seg√∫n el tipo de contenido.
    """
    
    def __init__(self):
        # Splitters especializados
        self.code_splitter = RecursiveCharacterTextSplitter.from_language(
            language=Language.PYTHON,
            chunk_size=800,
            chunk_overlap=100
        )
        
        self.markdown_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=[
                ("#", "Header 1"),
                ("##", "Header 2"),
                ("###", "Header 3"),
            ]
        )
        
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
    
    def detect_content_type(self, text: str) -> Literal["code", "markdown", "text"]:
        """Detecta el tipo de contenido"""
        
        # Detectar c√≥digo Python
        code_patterns = [
            r'def\s+\w+\s*\(',
            r'class\s+\w+',
            r'import\s+\w+',
            r'from\s+\w+\s+import'
        ]
        
        if any(re.search(pattern, text) for pattern in code_patterns):
            return "code"
        
        # Detectar Markdown
        if re.search(r'^#{1,6}\s+', text, re.MULTILINE):
            return "markdown"
        
        return "text"
    
    def chunk_document(self, document: Document) -> List[Document]:
        """Aplica chunking adaptativo"""
        
        content_type = self.detect_content_type(document.page_content)
        
        print(f"üìù Tipo detectado: {content_type}")
        
        if content_type == "code":
            chunks = self.code_splitter.split_documents([document])
        elif content_type == "markdown":
            # Markdown splitter retorna docs sin metadata original
            md_chunks = self.markdown_splitter.split_text(document.page_content)
            # A√±adir metadata original
            chunks = [
                Document(
                    page_content=chunk.page_content,
                    metadata={**document.metadata, **chunk.metadata}
                )
                for chunk in md_chunks
            ]
        else:
            chunks = self.text_splitter.split_documents([document])
        
        return chunks


# Ejemplos de uso
def test_adaptive_chunker():
    """Prueba el chunker adaptativo con diferentes tipos de contenido"""
    
    # Documento de c√≥digo
    code_doc = Document(
        page_content="""
def calculate_embeddings(texts: List[str]) -> np.ndarray:
    '''
    Calcula embeddings para una lista de textos.
    
    Args:
        texts: Lista de strings a embedear
        
    Returns:
        Array de embeddings
    '''
    embeddings = []
    for text in texts:
        embedding = model.encode(text)
        embeddings.append(embedding)
    return np.array(embeddings)

class VectorStore:
    def __init__(self, dimension: int):
        self.dimension = dimension
        self.vectors = []
    
    def add(self, vector: np.ndarray):
        if len(vector) != self.dimension:
            raise ValueError("Dimensi√≥n incorrecta")
        self.vectors.append(vector)
        """,
        metadata={"source": "embeddings.py", "type": "code"}
    )
    
    # Documento Markdown
    markdown_doc = Document(
        page_content="""
# Gu√≠a de RAG

## Introducci√≥n

RAG combina recuperaci√≥n y generaci√≥n.

## Componentes Principales

### Document Loaders
Cargan documentos de diversas fuentes.

### Text Splitters
Dividen documentos en chunks.

### Embeddings
Convierten texto en vectores.
        """,
        metadata={"source": "rag_guide.md", "type": "markdown"}
    )
    
    # Documento de texto plano
    text_doc = Document(
        page_content="""
        Los sistemas RAG han revolucionado la forma en que los modelos
        de lenguaje acceden a informaci√≥n externa. Al combinar b√∫squeda
        sem√°ntica con generaci√≥n de lenguaje natural, estos sistemas
        pueden proporcionar respuestas m√°s precisas y actualizadas.
        
        La clave del √©xito de un sistema RAG radica en la calidad de
        su pipeline de recuperaci√≥n. Esto incluye el chunking inteligente,
        embeddings de alta calidad, y estrategias de retrieval optimizadas.
        """,
        metadata={"source": "intro.txt", "type": "text"}
    )
    
    # Probar chunker adaptativo
    chunker = AdaptiveChunker()
    
    for doc in [code_doc, markdown_doc, text_doc]:
        print("\n" + "=" * 80)
        print(f"Procesando: {doc.metadata['source']}")
        print("=" * 80)
        
        chunks = chunker.chunk_document(doc)
        
        print(f"\n‚úÖ Chunks generados: {len(chunks)}\n")
        
        for i, chunk in enumerate(chunks, 1):
            print(f"Chunk {i}:")
            print(f"  Longitud: {len(chunk.page_content)} chars")
            print(f"  Metadata: {chunk.metadata}")
            print(f"  Preview: {chunk.page_content[:100]}...")
            print()


if __name__ == "__main__":
    test_adaptive_chunker()
```

---

## üöÄ T√©cnicas Avanzadas

### 1. **Propositional Chunking**

Convierte oraciones complejas en proposiciones at√≥micas.

```python
"""
Propositional Chunking: Descomponer oraciones en proposiciones simples
Basado en: https://github.com/NirDiamant/RAG_Techniques
"""

from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from typing import List


class PropositionalChunker:
    """Divide texto en proposiciones at√≥micas usando un LLM"""
    
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", """Eres un experto en descomponer oraciones complejas 
            en proposiciones simples y at√≥micas.
            
            Reglas:
            1. Cada proposici√≥n debe ser auto-contenida
            2. Mantener el significado original
            3. Usar lenguaje simple y directo
            4. Retornar una proposici√≥n por l√≠nea
            """),
            ("human", "Descomp√≥n este texto en proposiciones:\n\n{text}")
        ])
    
    def chunk(self, text: str) -> List[str]:
        """Convierte texto en proposiciones"""
        
        chain = self.prompt | self.llm
        response = chain.invoke({"text": text})
        
        # Dividir por l√≠neas y limpiar
        propositions = [
            line.strip() 
            for line in response.content.split("\n") 
            if line.strip() and not line.strip().startswith("-")
        ]
        
        return propositions


# Ejemplo de uso
if __name__ == "__main__":
    chunker = PropositionalChunker()
    
    complex_text = """
    RAG systems, which combine retrieval and generation, are particularly 
    effective for question-answering tasks because they can access external 
    knowledge bases, unlike traditional language models that rely solely on 
    their training data.
    """
    
    propositions = chunker.chunk(complex_text)
    
    print("Texto original:")
    print(complex_text)
    print("\nProposiciones:")
    for i, prop in enumerate(propositions, 1):
        print(f"{i}. {prop}")
```

**Resultado esperado**:
1. RAG systems combine retrieval and generation
2. RAG systems are effective for question-answering tasks
3. RAG systems can access external knowledge bases
4. Traditional language models rely solely on training data
5. Traditional language models cannot access external knowledge

### 2. **Context-Aware Chunking con Overlap Inteligente**

```python
"""
Overlap Inteligente: Preservar contexto entre chunks
"""

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document


class ContextAwareChunker:
    """Chunker que preserva contexto mediante overlap inteligente"""
    
    def __init__(self, chunk_size: int = 800, overlap_size: int = 200):
        self.chunk_size = chunk_size
        self.overlap_size = overlap_size
        
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=overlap_size,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
    
    def chunk_with_context(self, document: Document) -> List[Document]:
        """Crea chunks con metadata de contexto"""
        
        chunks = self.splitter.split_documents([document])
        
        # Enriquecer con metadata de contexto
        for i, chunk in enumerate(chunks):
            chunk.metadata.update({
                "chunk_index": i,
                "total_chunks": len(chunks),
                "has_previous": i > 0,
                "has_next": i < len(chunks) - 1,
                "previous_chunk_preview": chunks[i-1].page_content[-100:] if i > 0 else None,
                "next_chunk_preview": chunks[i+1].page_content[:100] if i < len(chunks) - 1 else None
            })
        
        return chunks


# Uso
chunker = ContextAwareChunker()
doc = Document(
    page_content="Texto largo...",
    metadata={"source": "doc.pdf"}
)

chunks = chunker.chunk_with_context(doc)

for chunk in chunks:
    print(f"Chunk {chunk.metadata['chunk_index'] + 1}/{chunk.metadata['total_chunks']}")
    if chunk.metadata['has_previous']:
        print(f"  Contexto previo: ...{chunk.metadata['previous_chunk_preview']}")
    print(f"  Contenido: {chunk.page_content[:100]}...")
    if chunk.metadata['has_next']:
        print(f"  Contexto siguiente: {chunk.metadata['next_chunk_preview']}...")
```

---

## ‚úÖ Mejores Pr√°cticas

### 1. **Experimentar con Tama√±os de Chunk**

```python
def find_optimal_chunk_size(documents: List[Document], 
                           test_queries: List[str],
                           chunk_sizes: List[int] = [200, 400, 800, 1200]):
    """
    Encuentra el tama√±o √≥ptimo de chunk mediante experimentaci√≥n
    """
    
    results = {}
    
    for size in chunk_sizes:
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=size,
            chunk_overlap=size // 4  # 25% overlap
        )
        
        chunks = splitter.split_documents(documents)
        
        # Aqu√≠ ir√≠an m√©tricas de evaluaci√≥n
        # (precision, recall, etc.)
        
        results[size] = {
            "num_chunks": len(chunks),
            "avg_chunk_length": sum(len(c.page_content) for c in chunks) / len(chunks)
        }
    
    return results
```

### 2. **Usar Overlap Apropiado**

```python
# ‚ùå Sin overlap - pierde contexto
splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=0  # Malo
)

# ‚úÖ Con overlap - preserva contexto
splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=200  # 25% overlap - recomendado
)
```

### 3. **Preservar Metadata Rica**

```python
def enrich_chunks_metadata(chunks: List[Document]) -> List[Document]:
    """Enriquece chunks con metadata √∫til"""
    
    for i, chunk in enumerate(chunks):
        chunk.metadata.update({
            "chunk_id": f"{chunk.metadata.get('source', 'unknown')}_{i}",
            "chunk_index": i,
            "chunk_length": len(chunk.page_content),
            "word_count": len(chunk.page_content.split()),
            "created_at": datetime.now().isoformat()
        })
    
    return chunks
```

---

## üéØ Resumen

### Lo que Aprendimos

‚úÖ **Chunking es cr√≠tico** para la calidad del RAG  
‚úÖ **Trade-off fundamental**: Precisi√≥n vs Contexto  
‚úÖ **M√∫ltiples estrategias**: Character, Recursive, Token, Semantic  
‚úÖ **Frameworks diversos**: LangChain, LlamaIndex  
‚úÖ **T√©cnicas avanzadas**: Propositional, Context-Aware  

### Checklist de Implementaci√≥n

- [ ] Definir tama√±o de chunk seg√∫n tu caso de uso
- [ ] Implementar overlap (recomendado: 20-25%)
- [ ] Elegir estrategia de splitting apropiada
- [ ] Enriquecer metadata de chunks
- [ ] Experimentar y medir resultados

### Pr√≥ximo Paso

Con tus chunks creados, el siguiente paso es convertirlos en **embeddings** (vectores num√©ricos) para b√∫squeda sem√°ntica.

‚û°Ô∏è **[Continuar a Parte 3: Embeddings](03_embeddings.md)**

---

<div align="center">

**[‚¨ÖÔ∏è Anterior: Document Loaders](01_document_loaders.md)** | **[Volver al M√≥dulo 5](README.md)** | **[Siguiente: Embeddings ‚û°Ô∏è](03_embeddings.md)**

</div>
