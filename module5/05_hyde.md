# Parte 5: HyDE - Hypothetical Document Embeddings

![HyDE](https://img.shields.io/badge/RAG_Pipeline-HyDE-9B59B6?style=for-the-badge)

## üìñ √çndice
1. [Fundamentos Conceptuales](#fundamentos-conceptuales)
2. [C√≥mo Funciona HyDE](#c√≥mo-funciona-hyde)
3. [Implementaci√≥n Pr√°ctica](#implementaci√≥n-pr√°ctica)
4. [Casos de Uso y Limitaciones](#casos-de-uso-y-limitaciones)

---

## üéØ Fundamentos Conceptuales

### El Problema que Resuelve HyDE

En RAG tradicional, embedeamos la **pregunta del usuario** y buscamos documentos similares. Pero las preguntas y las respuestas tienen estructuras ling√º√≠sticas muy diferentes:

```python
# Problema tradicional
query = "¬øC√≥mo funciona el aprendizaje por refuerzo?"
# Embedding de una pregunta (corta, interrogativa)

document = """
El aprendizaje por refuerzo es un paradigma de machine learning donde
un agente aprende a tomar decisiones mediante interacci√≥n con un entorno.
El agente recibe recompensas por acciones correctas y penalizaciones por
acciones incorrectas, optimizando su pol√≠tica a lo largo del tiempo.
"""
# Embedding de una respuesta (larga, declarativa)

# La similitud puede ser sub√≥ptima debido a la diferencia estructural
```

### La Soluci√≥n HyDE

**HyDE** genera un **documento hipot√©tico** que responde la pregunta, luego busca documentos similares a ese documento hipot√©tico.

```mermaid
graph TD
    A[Query: '¬øC√≥mo funciona RL?'] --> B[LLM]
    B --> C[Documento Hipot√©tico:<br/>'RL es un paradigma...']
    C --> D[Embedding]
    D --> E[Vector Search]
    E --> F[Documentos Reales Similares]
    
    style B fill:#9B59B6,stroke:#6C3483,stroke-width:3px,color:#fff
    style C fill:#F39C12,stroke:#D68910,stroke-width:2px
```

> [!IMPORTANT]
> **La clave de HyDE**: No importa si el documento hipot√©tico es factualmente correcto. Lo que importa es que tenga la **estructura ling√º√≠stica** y **vocabulario** similar a los documentos reales que contienen la respuesta.

---

## üß† C√≥mo Funciona HyDE

### Paso a Paso

1. **Usuario hace una pregunta**
   ```
   "¬øCu√°les son las mejores pr√°cticas para RAG?"
   ```

2. **LLM genera documento hipot√©tico**
   ```
   "Las mejores pr√°cticas para RAG incluyen: usar chunking sem√°ntico,
   implementar hybrid search combinando b√∫squeda vectorial con BM25,
   aplicar reranking con modelos cross-encoder, y mantener metadata rica..."
   ```

3. **Embedear documento hipot√©tico**
   ```python
   hyp_embedding = embed(hypothetical_document)
   ```

4. **Buscar documentos similares al hipot√©tico**
   ```python
   real_docs = vector_db.search(hyp_embedding, k=5)
   ```

5. **Usar documentos reales como contexto**
   ```python
   answer = llm.generate(query, context=real_docs)
   ```

### Comparaci√≥n: RAG vs HyDE

| Aspecto | RAG Tradicional | HyDE |
|---------|-----------------|------|
| **Embedding** | Pregunta del usuario | Documento hipot√©tico |
| **Estructura** | Interrogativa, corta | Declarativa, detallada |
| **Vocabulario** | Puede no coincidir | M√°s similar a docs reales |
| **Precisi√≥n** | Buena | Mejor (10-20% mejora) |
| **Latencia** | Baja | Media (requiere LLM call extra) |
| **Costo** | Bajo | Medio (LLM call adicional) |

---

## üíª Implementaci√≥n Pr√°ctica

### Ejemplo 1: HyDE B√°sico con LangChain

```python
"""
Ejemplo B√°sico: HyDE con LangChain
Framework: LangChain
Objetivo: Implementar HyDE para mejorar retrieval
"""

from langchain.chains import HypotheticalDocumentEmbedder
from langchain_openai import OpenAI, OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain_chroma import Chroma


# 1. Setup base embeddings
base_embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# 2. Setup LLM para generar documentos hipot√©ticos
llm = OpenAI(model="gpt-3.5-turbo-instruct", temperature=0)

# 3. Prompt para generar documento hipot√©tico
prompt_template = """
Por favor, escribe un p√°rrafo que responda la siguiente pregunta:

Pregunta: {question}

P√°rrafo:
"""

prompt = PromptTemplate(
    input_variables=["question"],
    template=prompt_template
)

# 4. Crear HyDE embedder
hyde_embeddings = HypotheticalDocumentEmbedder(
    llm_chain=prompt | llm,
    base_embeddings=base_embeddings
)

# 5. Crear vectorstore con HyDE
documents = [
    "RAG combina recuperaci√≥n de informaci√≥n con generaci√≥n de lenguaje natural.",
    "Los embeddings vectoriales capturan el significado sem√°ntico del texto.",
    "Hybrid search combina b√∫squeda vectorial con b√∫squeda por keywords.",
]

vectorstore = Chroma.from_texts(
    documents,
    hyde_embeddings  # Usar HyDE embeddings
)

# 6. B√∫squeda con HyDE
query = "¬øQu√© es RAG?"

# Internamente:
# 1. LLM genera: "RAG es una t√©cnica que combina..."
# 2. Embedea el documento hipot√©tico
# 3. Busca documentos similares

results = vectorstore.similarity_search(query, k=2)

print("Resultados con HyDE:")
for i, doc in enumerate(results, 1):
    print(f"{i}. {doc.page_content}")
```

### Ejemplo 2: HyDE Multi-Shot (M√∫ltiples Hip√≥tesis)

```python
"""
Ejemplo Intermedio: Multi-Shot HyDE
Objetivo: Generar m√∫ltiples documentos hipot√©ticos y promediar embeddings
"""

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain_chroma import Chroma
import numpy as np
from typing import List


class MultiShotHyDE:
    """
    HyDE que genera m√∫ltiples documentos hipot√©ticos
    y promedia sus embeddings para mayor robustez.
    """
    
    def __init__(self, num_hypotheses: int = 3):
        self.num_hypotheses = num_hypotheses
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", """Eres un experto que genera respuestas detalladas.
            Genera un p√°rrafo informativo que responda la pregunta."""),
            ("human", "{question}")
        ])
    
    def generate_hypotheses(self, query: str) -> List[str]:
        """Genera m√∫ltiples documentos hipot√©ticos"""
        
        hypotheses = []
        
        for i in range(self.num_hypotheses):
            chain = self.prompt | self.llm
            response = chain.invoke({"question": query})
            hypotheses.append(response.content)
        
        return hypotheses
    
    def embed_query(self, query: str) -> List[float]:
        """Embedea query usando promedio de m√∫ltiples hip√≥tesis"""
        
        # Generar hip√≥tesis
        hypotheses = self.generate_hypotheses(query)
        
        print(f"üìù Generadas {len(hypotheses)} hip√≥tesis:")
        for i, hyp in enumerate(hypotheses, 1):
            print(f"\n{i}. {hyp[:100]}...")
        
        # Embedear cada hip√≥tesis
        hypothesis_embeddings = self.embeddings.embed_documents(hypotheses)
        
        # Promediar embeddings
        avg_embedding = np.mean(hypothesis_embeddings, axis=0)
        
        return avg_embedding.tolist()


# Uso
if __name__ == "__main__":
    # Crear documentos
    documents = [
        "Machine learning es un subcampo de la inteligencia artificial",
        "Deep learning usa redes neuronales profundas",
        "RAG combina retrieval con generation para mejorar respuestas"
    ]
    
    # Setup vectorstore normal
    normal_embeddings = OpenAIEmbeddings()
    vectorstore = Chroma.from_texts(documents, normal_embeddings)
    
    # Query
    query = "¬øQu√© es machine learning?"
    
    # B√∫squeda con Multi-Shot HyDE
    multi_hyde = MultiShotHyDE(num_hypotheses=3)
    hyde_embedding = multi_hyde.embed_query(query)
    
    # Buscar usando embedding de HyDE
    results = vectorstore.similarity_search_by_vector(hyde_embedding, k=2)
    
    print("\n" + "="*80)
    print("RESULTADOS:")
    print("="*80)
    for i, doc in enumerate(results, 1):
        print(f"\n{i}. {doc.page_content}")
```

### Ejemplo 3: HyDE Adaptativo con Fallback

```python
"""
Ejemplo Avanzado: HyDE Adaptativo
Objetivo: Usar HyDE solo cuando mejora resultados, sino usar RAG tradicional
"""

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain_chroma import Chroma
from langchain.schema import Document
import numpy as np
from typing import List, Tuple


class AdaptiveHyDE:
    """
    Sistema que decide autom√°ticamente si usar HyDE o RAG tradicional
    basado en la confianza de los resultados.
    """
    
    def __init__(self, confidence_threshold: float = 0.75):
        self.confidence_threshold = confidence_threshold
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        
        self.hyde_prompt = ChatPromptTemplate.from_messages([
            ("system", "Genera un p√°rrafo detallado que responda esta pregunta:"),
            ("human", "{question}")
        ])
    
    def traditional_search(
        self, 
        vectorstore: Chroma, 
        query: str, 
        k: int = 5
    ) -> Tuple[List[Document], List[float]]:
        """B√∫squeda RAG tradicional"""
        
        results = vectorstore.similarity_search_with_score(query, k=k)
        docs = [doc for doc, _ in results]
        scores = [score for _, score in results]
        
        return docs, scores
    
    def hyde_search(
        self, 
        vectorstore: Chroma, 
        query: str, 
        k: int = 5
    ) -> Tuple[List[Document], List[float]]:
        """B√∫squeda con HyDE"""
        
        # Generar documento hipot√©tico
        chain = self.hyde_prompt | self.llm
        response = chain.invoke({"question": query})
        hypothetical_doc = response.content
        
        print(f"üìÑ Documento hipot√©tico generado:")
        print(f"   {hypothetical_doc[:150]}...\n")
        
        # Embedear documento hipot√©tico
        hyp_embedding = self.embeddings.embed_query(hypothetical_doc)
        
        # Buscar
        results = vectorstore.similarity_search_by_vector_with_relevance_scores(
            hyp_embedding, 
            k=k
        )
        
        docs = [doc for doc, _ in results]
        scores = [score for _, score in results]
        
        return docs, scores
    
    def adaptive_search(
        self, 
        vectorstore: Chroma, 
        query: str, 
        k: int = 5
    ) -> Tuple[List[Document], str]:
        """
        B√∫squeda adaptativa: intenta tradicional primero,
        usa HyDE si confianza es baja.
        """
        
        # Intentar b√∫squeda tradicional
        trad_docs, trad_scores = self.traditional_search(vectorstore, query, k)
        
        # Evaluar confianza (score del mejor resultado)
        best_score = max(trad_scores) if trad_scores else 0
        
        print(f"üîç B√∫squeda tradicional - Mejor score: {best_score:.4f}")
        
        # Si confianza es alta, usar resultados tradicionales
        if best_score >= self.confidence_threshold:
            print("‚úÖ Confianza alta - Usando RAG tradicional\n")
            return trad_docs, "traditional"
        
        # Si confianza es baja, usar HyDE
        print("‚ö†Ô∏è  Confianza baja - Activando HyDE...\n")
        hyde_docs, hyde_scores = self.hyde_search(vectorstore, query, k)
        
        hyde_best_score = max(hyde_scores) if hyde_scores else 0
        print(f"üéØ HyDE - Mejor score: {hyde_best_score:.4f}")
        
        # Comparar y retornar mejor opci√≥n
        if hyde_best_score > best_score:
            print("‚úÖ HyDE mejor√≥ resultados\n")
            return hyde_docs, "hyde"
        else:
            print("‚ÑπÔ∏è  HyDE no mejor√≥, usando tradicional\n")
            return trad_docs, "traditional"


# Demostraci√≥n
if __name__ == "__main__":
    # Crear documentos
    documents = [
        Document(
            page_content="El aprendizaje por refuerzo es un paradigma de ML donde un agente aprende mediante interacci√≥n con un entorno.",
            metadata={"source": "ml_book.pdf"}
        ),
        Document(
            page_content="Las redes neuronales convolucionales son efectivas para procesamiento de im√°genes.",
            metadata={"source": "dl_guide.pdf"}
        ),
        Document(
            page_content="RAG combina recuperaci√≥n de informaci√≥n con generaci√≥n para respuestas m√°s precisas.",
            metadata={"source": "rag_paper.pdf"}
        ),
    ]
    
    # Crear vectorstore
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma.from_documents(documents, embeddings)
    
    # Sistema adaptativo
    adaptive_hyde = AdaptiveHyDE(confidence_threshold=0.75)
    
    # Queries de prueba
    queries = [
        "¬øQu√© es reinforcement learning?",  # Deber√≠a usar HyDE (t√©rminos t√©cnicos)
        "RAG systems",  # Deber√≠a usar tradicional (match directo)
    ]
    
    for query in queries:
        print("=" * 80)
        print(f"QUERY: {query}")
        print("=" * 80 + "\n")
        
        results, method = adaptive_hyde.adaptive_search(vectorstore, query, k=2)
        
        print(f"M√©todo usado: {method.upper()}")
        print("\nResultados:")
        for i, doc in enumerate(results, 1):
            print(f"{i}. {doc.page_content[:100]}...")
        
        print("\n")
```

---

## üéØ Casos de Uso y Limitaciones

### Cu√°ndo Usar HyDE

‚úÖ **Queries t√©cnicas con terminolog√≠a espec√≠fica**
```python
query = "¬øC√≥mo implementar backpropagation en una red neuronal?"
# HyDE genera respuesta t√©cnica con vocabulario correcto
```

‚úÖ **Preguntas conceptuales amplias**
```python
query = "Explica el concepto de overfitting"
# HyDE genera explicaci√≥n detallada similar a documentaci√≥n
```

‚úÖ **B√∫squedas en dominios especializados**
```python
query = "¬øCu√°les son los s√≠ntomas de la diabetes tipo 2?"
# HyDE genera respuesta m√©dica con terminolog√≠a apropiada
```

### Cu√°ndo NO Usar HyDE

‚ùå **B√∫squedas de hechos espec√≠ficos**
```python
query = "¬øCu√°l es la capital de Francia?"
# RAG tradicional es suficiente y m√°s r√°pido
```

‚ùå **Queries con keywords exactos**
```python
query = "error: ModuleNotFoundError numpy"
# Mejor usar keyword search (BM25)
```

‚ùå **Restricciones de latencia estrictas**
```python
# HyDE a√±ade ~500ms-1s por el LLM call adicional
# Si necesitas <100ms, usa RAG tradicional
```

### Limitaciones

1. **Costo adicional**: Requiere LLM call extra
2. **Latencia**: A√±ade tiempo de generaci√≥n
3. **Alucinaciones**: Documento hipot√©tico puede ser incorrecto (pero a√∫n √∫til)
4. **Dependencia de LLM**: Calidad depende del modelo usado

---

## üéØ Resumen

### Lo que Aprendimos

‚úÖ **HyDE** genera documentos hipot√©ticos para mejorar retrieval  
‚úÖ **Mejora 10-20%** en precisi√≥n para queries complejas  
‚úÖ **Multi-Shot HyDE** usa m√∫ltiples hip√≥tesis para robustez  
‚úÖ **HyDE Adaptativo** decide cu√°ndo usar HyDE vs RAG tradicional  
‚úÖ **Trade-off**: Mejor precisi√≥n vs mayor latencia/costo  

### Pr√≥ximo Paso

Con retrieval optimizado mediante HyDE, el siguiente paso es implementar **sistemas de memoria** para conversaciones contextuales.

‚û°Ô∏è **[Continuar a Parte 6: Memory Systems](06_memory.md)**

---

<div align="center">

**[‚¨ÖÔ∏è Anterior: Vector Databases](04_vector_databases.md)** | **[Volver al M√≥dulo 5](README.md)** | **[Siguiente: Memory ‚û°Ô∏è](06_memory.md)**

</div>
