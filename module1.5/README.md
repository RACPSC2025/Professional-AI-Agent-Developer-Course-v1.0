# ğŸŒ MÃ³dulo 1.5: Ecosistemas Open Source (HuggingFace & Ollama)

![Module 1.5 Banner](../images/module1.5_banner.png)

![Level](https://img.shields.io/badge/Nivel-Intermedio-C3B1E1?style=for-the-badge&logo=python&logoColor=white)
![Time](https://img.shields.io/badge/Tiempo-4_Horas-A7C7E7?style=for-the-badge&labelColor=2D2D44)
![Stack](https://img.shields.io/badge/Stack-HuggingFace_&_Ollama-FFD21E?style=for-the-badge)

> *"El verdadero poder de la IA no estÃ¡ solo en los modelos mÃ¡s grandes, sino en la accesibilidad y la comunidad. Open Source es el motor de la innovaciÃ³n."*

---

## ğŸ¯ VisiÃ³n General

Antes de sumergirnos en los frameworks de agentes (MÃ³dulo 2), es crucial entender dÃ³nde viven los modelos y cÃ³mo ejecutarlos. No siempre dependeremos de APIs cerradas como OpenAI o Anthropic.

En este mÃ³dulo puente, dominaremos los dos pilares del ecosistema Open Source:
1.  **HuggingFace**: El "GitHub" de la Inteligencia Artificial.
2.  **Ollama**: La forma mÃ¡s sencilla de ejecutar LLMs localmente.

> [!NOTE]
> **Objetivo del MÃ³dulo**: Aprender a encontrar, evaluar y ejecutar modelos Open Source (Llama 3, Mistral, Gemma) tanto en la nube (Inference API) como en tu propia mÃ¡quina (Ollama/Transformers).

---

## ğŸ§  Parte I: HuggingFace Hub

### Â¿QuÃ© es HuggingFace?
HuggingFace es la plataforma central de la comunidad de IA. Aloja mÃ¡s de 500,000 modelos, datasets y demos. Es el estÃ¡ndar de facto para compartir ML.

### Componentes Clave

1.  **The Hub**: Repositorio central.
    *   **Model Cards**: DocumentaciÃ³n tÃ©cnica del modelo (uso, limitaciones, entrenamiento).
    *   **Files**: Los pesos del modelo (`.safetensors`, `.bin`).
    *   **Community**: Discusiones y Pull Requests sobre modelos.

2.  **Transformers Library**: La librerÃ­a de Python mÃ¡s popular para descargar y usar estos modelos.
    ```python
    from transformers import pipeline
    pipe = pipeline("text-generation", model="gpt2")
    ```

3.  **Inference API (Serverless)**: Permite probar modelos vÃ­a API HTTP sin GPU local. Ideal para prototipos rÃ¡pidos y gratuitos (con rate limits).

### ğŸ” CÃ³mo leer una Model Card
Cuando entras a un modelo (ej. `meta-llama/Meta-Llama-3-8B`), busca:
*   **Model Description**: Â¿QuÃ© hace? Â¿Es base o instruct/chat?
*   **Intended Use**: Â¿Para quÃ© fue entrenado?
*   **Prompt Template**: Â¿CÃ³mo debo formatear el texto? (e.g., `<|user|>...`)
*   **License**: Â¿Puedo usarlo comercialmente? (Apache 2.0, MIT, Community License).

---

## ğŸ¦™ Parte II: Ollama (Local Runtime)

### Â¿QuÃ© es Ollama?
Ollama es una herramienta que empaqueta modelos LLM en contenedores ligeros (similar a Docker) para ejecutarlos localmente de forma extremadamente sencilla.

### Â¿Por quÃ© Ollama?
*   **Privacidad Total**: Los datos nunca salen de tu mÃ¡quina.
*   **Costo Cero**: Usas tu propio hardware (CPU/GPU).
*   **Latencia Cero**: Sin llamadas de red a internet.
*   **Simplicidad**: `ollama run llama3` y listo.

### Arquitectura
*   **Ollama Server**: Un proceso en background que gestiona la carga del modelo en RAM/VRAM.
*   **Modelfile**: Archivo de configuraciÃ³n (como Dockerfile) que define el modelo base, parÃ¡metros y system prompt.
*   **API**: Expone una API REST compatible con OpenAI.

### Comandos Esenciales
```bash
ollama pull llama3       # Descargar modelo
ollama run llama3        # Ejecutar chat interactivo
ollama list              # Ver modelos instalados
ollama rm llama3         # Eliminar modelo
```

---

## âš”ï¸ Comparativa: Cloud vs Local vs HÃ­brido

| CaracterÃ­stica | API Comercial (OpenAI/Anthropic) | HuggingFace Inference API | Local (Ollama/Transformers) |
| :--- | :--- | :--- | :--- |
| **Costo** | $$$ (por token) | Gratis (limitado) / $$ (Pro) | Gratis (hardware propio) |
| **Privacidad** | Datos viajan a terceros | Datos viajan a HF | **Privacidad Total** ğŸ”’ |
| **Calidad** | SOTA (GPT-4, Claude 3.5) | Variable (depende del modelo) | Depende del hardware (7B-70B) |
| **Latencia** | Media (red) | Media (red + cola) | **Baja** (local) âš¡ |
| **Setup** | InstantÃ¡neo (API Key) | InstantÃ¡neo (Token) | Requiere instalaciÃ³n + RAM |
| **Uso Ideal** | ProducciÃ³n compleja, razonamiento alto | Prototipos, demos | Desarrollo, datos sensibles, offline |

---

## ğŸ› ï¸ Proyectos PrÃ¡cticos

### ğŸŸ¢ Proyecto 1: HuggingFace Inference API
Usaremos la API gratuita de HF para tareas de clasificaciÃ³n y generaciÃ³n sin descargar nada.
ğŸ“„ [01_huggingface_inference.py](./examples/01_huggingface_inference.py)

### ğŸŸ¡ Proyecto 2: Transformers Pipeline Local
Descargaremos un modelo pequeÃ±o (GPT-2 o TinyLlama) usando la librerÃ­a `transformers` para entender cÃ³mo funciona "bajo el capÃ³".
ğŸ“„ [02_transformers_pipeline.py](./examples/02_transformers_pipeline.py)

### ğŸŸ¡ Proyecto 3: Chatbot Local con Ollama
Interactuaremos con un modelo Llama 3 corriendo en tu mÃ¡quina a travÃ©s de Python.
ğŸ“„ [03_ollama_setup.py](./examples/03_ollama_setup.py)

### ğŸ”´ Proyecto 4: Agente HÃ­brido
Un agente que usa un modelo local (Ollama) para resumir textos privados y un modelo en la nube (HF/OpenAI) para tareas de conocimiento general.
ğŸ“„ [04_hybrid_agent.py](./examples/04_hybrid_agent.py)

---

## ğŸ“š Recursos Adicionales

- [HuggingFace Hub](https://huggingface.co/models)
- [Ollama Website](https://ollama.com/)
- [Curso de NLP de HuggingFace](https://huggingface.co/learn/nlp-course/chapter1/1)
- [LangChain + Ollama](https://python.langchain.com/docs/integrations/llms/ollama/)

---

<div align="center">

**[â¬…ï¸ MÃ³dulo 1: Panorama LLMs](../module1/README.md)** | **[ğŸ  Inicio](../README.md)** | **[Siguiente MÃ³dulo 2: Frameworks â¡ï¸](../module2/README.md)**

</div>
