"""
Ejemplo 2: Transformers Pipeline (Local Inference)
Nivel: üü° Intermedio
Objetivo: Descargar y ejecutar un modelo peque√±o localmente usando `transformers`.

A diferencia de la API, aqu√≠ el modelo corre en TU m√°quina (CPU/GPU).
Usaremos 'GPT-2' (muy peque√±o) para asegurar que corra en cualquier PC.

Requisitos:
- pip install transformers torch
"""

from transformers import pipeline, set_seed
import warnings

# Suprimir warnings de transformers para limpiar output
warnings.filterwarnings("ignore")

def run_local_pipeline():
    print("\n" + "="*60)
    print("üíª EJEMPLO 2: Transformers Pipeline (Local)")
    print("="*60 + "\n")

    # 1. Selecci√≥n del Modelo
    # Usamos GPT-2 porque es muy ligero (~500MB) y corre r√°pido en CPU.
    # Para mejores resultados (si tienes GPU), prueba 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'
    model_id = "gpt2" 
    
    print(f"üì• Descargando/Cargando modelo '{model_id}' en memoria local...")
    print("   (La primera vez esto tomar√° unos segundos/minutos)\n")

    # 2. Crear el Pipeline
    # 'text-generation' abstrae toda la complejidad (tokenizaci√≥n, modelo, decoding)
    generator = pipeline('text-generation', model=model_id)
    set_seed(42) # Para reproducibilidad

    # 3. Ejecutar Inferencia
    prompt = "The future of Artificial Intelligence is"
    
    print(f"üìù Prompt: '{prompt}'")
    print("‚öôÔ∏è  Generando...\n")

    outputs = generator(
        prompt, 
        max_length=50, 
        num_return_sequences=3,
        truncation=True
    )

    # 4. Mostrar Resultados
    print("‚ú® Resultados Generados:")
    for i, output in enumerate(outputs):
        print(f"\nOpci√≥n {i+1}:")
        print(f"'{output['generated_text']}'")

    print("\n" + "="*60)
    print("‚úÖ Ventajas Local: Privacidad total, sin internet, costo cero.")
    print("‚ùå Desventajas Local: Consume RAM/CPU, descarga inicial.")

if __name__ == "__main__":
    run_local_pipeline()
