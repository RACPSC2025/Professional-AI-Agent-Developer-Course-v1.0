# MÃ³dulo 13: Testing de Agentes

## ðŸŽ¯ Objetivos del MÃ³dulo

Aprender a testear AI Agents de forma profesional, desde unit tests hasta pipelines CI/CD completos.

## ðŸ“š Conceptos Clave

### 1. Unit Testing

**Concepto:** Testear componentes individuales de agentes de forma aislada

**DesafÃ­o Testing Ãºnico de LLMs:**
- Outputs no determinÃ­sticos
- Necesidad de mocking
- EvaluaciÃ³n de calidad (no solo igualdad exacta)

**Soluciones:**
- Mock LLM calls para tests determinÃ­sticos  
- LLM-as-a-Judge para evaluar calidad
- Similarity metrics
- Gold standard datasets

### 2. Integration Testing

**Concepto:** Testear flujos completos de mÃºltiples agentes

**QuÃ© testear:**
- ComunicaciÃ³n entre agentes
- Manejo de errores en cascada
- Performance end-to-end
- Estado compartido correctamente

### 3. CI/CD para Agentes

**Diferencia con CI/CD tradicional:**
- Evaluation metrics vs traditional assertions
- LLM-powered tests
- Dataset versioning
- Prompt versioning

## ðŸ› ï¸ Proyectos PrÃ¡cticos

### ðŸŸ¢ Nivel BÃ¡sico: Unit Testing Agents
**Archivo:** `01_unit_testing_agents.py`
- **Framework:** pytest con mocking
- **Concepto:** Tests determinÃ­sticos con LLM mocks
- **Coverage:** Funciones, tools, prompts

### ðŸŸ¡ Nivel Intermedio: Integration Testing Multi-Agent
**Archivo:** `02_integration_testing_multiagent.py`
- **Framework:** pytest con fixtures
- **Concepto:** Test workflows completos
- **Caso de uso:** Sistema multi-agente research â†’ analysis â†’ report

### ðŸ”´ Nivel Avanzado: CI/CD Pipeline
**Archivo:** `03_cicd_pipeline_agents.py`
- **Framework:** GitHub Actions + pytest
- **Concepto:** Automated testing en cada commit/PR
- **Includes:** Regression tests, performance benchmarks

## ðŸŽ“ Best Practices

### Testing Pyramid para Agentes

```
        /\
       /  \  E2E Tests (5%)
      /    \  Integration Tests (25%)
     /      \ Unit Tests (70%)
    /________\
```

### MÃ©tricas Clave

- **Test Coverage:** >80% de funciones
- **Response Quality:** Score >0.8 en evaluaciones
- **Latency:** p95 < threshold
- **Cost:** $ per test run

### Golden Rules

1. **Mock LLMs en unit tests** (rÃ¡pido,  barato, determinÃ­stico)
2. **Use real LLMs en integration** (catch real issues)
3. **Version prompts** (git track cambios)
4. **Maintain gold datasets** (regression detection)
5. **Automate everything** (CI/CD esencial)

## ðŸ“Š Test Example Pattern

```python
def test_agent_response():
    # Arrange
    llm_mock = Mock()
    llm_mock.invoke.return_value = "Expected output"
    agent = MyAgent(llm=llm_mock)
    
    # Act
    result = agent.process("test input")
    
    # Assert
    assert result == "Expected output"
    llm_mock.invoke.assert_called_once()
```

## ðŸš€ Quick Start

```bash
# Install dependencies
pip install pytest pytest-cov pytest-asyncio

# Run tests
pytest tests/ -v

# With coverage
pytest tests/ --cov=agents --cov-report=html

# Specific test
pytest tests/test_agent.py::test_specific_function
```

## ðŸ“š Recursos

- [pytest Documentation](https://docs.pytest.org)
- [unittest.mock Guide](https://docs.python.org/3/library/unittest.mock.html)
- [GitHub Actions for Python](https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python)
