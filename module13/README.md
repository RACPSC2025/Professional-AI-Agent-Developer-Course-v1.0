# M√≥dulo 13: Testing de Agentes

## üéØ Objetivos del M√≥dulo

Aprender a testear AI Agents de forma profesional, desde unit tests hasta pipelines CI/CD completos.

## üìö Conceptos Clave

### 1. Unit Testing

**Concepto:** Testear componentes individuales de agentes de forma aislada

**Desaf√≠o Testing √∫nico de LLMs:**
- Outputs no determin√≠sticos
- Necesidad de mocking
- Evaluaci√≥n de calidad (no solo igualdad exacta)

**Soluciones:**
- Mock LLM calls para tests determin√≠sticos  
- LLM-as-a-Judge para evaluar calidad
- Similarity metrics
- Gold standard datasets

### 2. Integration Testing

**Concepto:** Testear flujos completos de m√∫ltiples agentes

**Qu√© testear:**
- Comunicaci√≥n entre agentes
- Manejo de errores en cascada
- Performance end-to-end
- Estado compartido correctamente

### 3. CI/CD para Agentes

**Diferencia con CI/CD tradicional:**
- Evaluation metrics vs traditional assertions
- LLM-powered tests
- Dataset versioning
- Prompt versioning

## üõ†Ô∏è Proyectos Pr√°cticos

### üü¢ Nivel B√°sico: Unit Testing Agents
**Archivo:** `01_unit_testing_agents.py`
- **Framework:** pytest con mocking
- **Concepto:** Tests determin√≠sticos con LLM mocks
- **Coverage:** Funciones, tools, prompts

### üü° Nivel Intermedio: Integration Testing Multi-Agent
**Archivo:** `02_integration_testing_multiagent.py`
- **Framework:** pytest con fixtures
- **Concepto:** Test workflows completos
- **Caso de uso:** Sistema multi-agente research ‚Üí analysis ‚Üí report

### üî¥ Nivel Avanzado: CI/CD Pipeline
**Archivo:** `03_cicd_pipeline_agents.py`
- **Framework:** GitHub Actions + pytest
- **Concepto:** Automated testing en cada commit/PR
- **Includes:** Regression tests, performance benchmarks

## üéì Best Practices

### Testing Pyramid para Agentes

```
        /\
       /  \  E2E Tests (5%)
      /    \  Integration Tests (25%)
     /      \ Unit Tests (70%)
    /________\
```

### M√©tricas Clave

- **Test Coverage:** >80% de funciones
- **Response Quality:** Score >0.8 en evaluaciones
- **Latency:** p95 < threshold
- **Cost:** $ per test run

### Golden Rules

1. **Mock LLMs en unit tests** (r√°pido,  barato, determin√≠stico)
2. **Use real LLMs en integration** (catch real issues)
3. **Version prompts** (git track cambios)
4. **Maintain gold datasets** (regression detection)
5. **Automate everything** (CI/CD esencial)

## üìä Test Example Pattern

```python
def test_agent_response():
    # Arrange
    llm_mock = Mock()
    llm_mock.invoke.return_value = "Expected output"
    agent = MyAgent(llm=llm_mock)
    
    # Act
    result = agent.process("test input")
    
    # Assert
    assert result == "Expected output"
    llm_mock.invoke.assert_called_once()
```

## üöÄ Quick Start

```bash
# Install dependencies
pip install pytest pytest-cov pytest-asyncio

# Run tests
pytest tests/ -v

# With coverage
pytest tests/ --cov=agents --cov-report=html

# Specific test
pytest tests/test_agent.py::test_specific_function
```

## üìö Recursos

- [pytest Documentation](https://docs.pytest.org)
- [unittest.mock Guide](https://docs.python.org/3/library/unittest.mock.html)
- [GitHub Actions for Python](https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python)

---

<div align="center">
<a href="../module14/README.md">‚û°Ô∏è Siguiente M√≥dulo: Deployment & DevOps</a>
</div>
